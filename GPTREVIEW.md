# GPT Review

## Structure Overview
- `app.py` bootstraps the Flask app, background scheduler, and DI container; it is the entry point all HTTP calls hit.
- `nasdaq_predictor/` is the main package: `analysis/` holds strategy math (block segmentation, signals, volatility), `services/` orchestrate workflows, `database/` wraps Supabase repositories/models, `api/` exposes Flask blueprints, `scheduler/` holds APScheduler jobs.
- `tests/` contains the regression suite split into `unit/` and `integration/`, with newer block prediction/verifications specs living under `tests/unit/services` and `tests/integration`.
- Supporting assets live under `templates/`, `static/`, and `scripts/` while numerous phase-specific Markdown reports document milestones in the repo root.

## Issues, Bugs & Duplication
1. **Fetcher contract mismatch breaks block predictions** – `BlockPredictionService.generate_block_prediction` always calls `_fetch_hourly_bars` → `fetcher.fetch_historical_data` to obtain a list of dicts (nasdaq_predictor/services/block_prediction_service.py:114-175, 215-263). All service tests and mocks expect the fetcher to expose `fetch_5min_ohlc` returning a pandas `DataFrame` (tests/unit/services/test_block_prediction_service.py:60-170), so every test invocation raises `'Mock' object is not iterable` and the service returns `None`. The code and tests now model two different data contracts.
2. **Incomplete `BlockPrediction` construction** – `_create_block_prediction` never populates the required `reference_price` field and therefore cannot instantiate the dataclass without raising a `TypeError` (nasdaq_predictor/services/block_prediction_service.py:288-349 vs nasdaq_predictor/database/models/block_prediction.py:52-113). As a side-effect the verification code later has to reach into `reference_levels` to find an opening price instead of trusting the model.
3. **Repository lookups use the wrong key** – Both `get_hourly_prediction` and `get_hourly_predictions_24h` pass the human-readable ticker symbol directly into `BlockPredictionRepository` (nasdaq_predictor/services/block_prediction_service.py:351-390). The repository tables are keyed by Supabase UUID (`ticker_id`), so these queries will miss every record unless the symbol happens to match the UUID string.
4. **Verification fetch pipeline is a stub** – `_fetch_blocks_6_7_bars` in `BlockVerificationService` always returns an empty list (nasdaq_predictor/services/block_verification_service.py:263-290). Because `verify_block_prediction` aborts when no bars are returned (lines 86-138) the verification flow can never succeed in production; integration tests currently fail with this exact scenario.
5. **Repository API is misused during verification** – `verify_block_prediction` calls `block_prediction_repo.update_verification` without the required `blocks_6_7_close` argument and passes directional strings like `"UP"` straight into `actual_result` (nasdaq_predictor/services/block_verification_service.py:102-138) even though the repository method (nasdaq_predictor/database/repositories/block_prediction_repository.py:229-295) is designed to store `'CORRECT'/'WRONG'` accuracy flags. The service also assumes the repository returns an updated `BlockPrediction`, but the method returns a boolean, so downstream consumers will operate on `True/False` objects rather than models.
6. **Batch verification plumbing is inconsistent** – `verify_pending_predictions` calls `get_pending_verifications(limit=limit)` even though the repository signature provides no `limit` parameter (nasdaq_predictor/services/block_verification_service.py:167 vs nasdaq_predictor/database/repositories/block_prediction_repository.py:229-257). The method returns stats keys `{'verified', 'correct', ...}` while the tests expect `'total_verified'`, `'correct_count'`, etc. (tests/unit/services/test_block_verification_service.py:177-209), so the observable contract is divergent in two places.
7. **Accuracy reporting uses incompatible arguments** – `get_verification_accuracy` passes a `date` kwarg into `BlockPredictionRepository.get_accuracy_metrics`, but that method only accepts a rolling `hours` window (nasdaq_predictor/services/block_verification_service.py:200-225 vs nasdaq_predictor/database/repositories/block_prediction_repository.py:300-344). Combined with issue #5 (storing `"UP"/"DOWN"` instead of correctness strings), the accuracy numbers cannot be trusted.
8. **Duplication/out-of-sync repository APIs** – The test suite still references legacy method names (`save`, `get_daily_predictions`, `get_by_hour`, `get_unverified` and a `fetch_5min_ohlc` fetcher) throughout tests/unit/services/test_block_prediction_service.py:65-206 and tests/unit/services/test_block_verification_service.py:87-209, while the concrete repository exposes `store_block_prediction`, `get_block_predictions_by_date`, `get_block_prediction_by_hour`, and `get_pending_verifications`. Maintaining both naming schemes in mocks and production code causes duplicated logic branches and keeps the suite red.

## Phase Plan
### Phase 1 – Stabilize block prediction generation
- Introduce a single fetcher contract: either add a thin `fetch_5min_ohlc` adapter on `YahooFinanceDataFetcher` that returns DataFrames for tests or update the service/tests to accept the existing list-of-dicts output. Normalize `_fetch_hourly_bars` so mocks can supply deterministic data and ensure pandas inputs are converted once near the service boundary.
- Fix `_create_block_prediction` to populate `reference_price`, store the human-readable ticker symbol when helpful, and surface any additional metadata (`blocks_6_7_close`, `actual_direction`) the verification pipeline needs later.
- Update `get_hourly_prediction` and `get_hourly_predictions_24h` to reuse `_resolve_ticker_uuid` (with caching) so lookups hit Supabase records. Backfill repository adapters or shims so the tests stop mocking nonexistent methods (`save`, `get_daily_predictions`, etc.).
- Verification: run `./venv/bin/pytest tests/unit/services/test_block_prediction_service.py` and `tests/integration/test_block_prediction_flow.py` to confirm the generation path passes again.

### Phase 2 – Repair verification and repository contracts
- Implement `_fetch_blocks_6_7_bars` by reusing the shared fetcher (prefer Supabase via `market_data_repo` with a yfinance fallback) so verification receives real OHLC slices. Honor timezone handling identical to the generation path.
- Align repository contracts: extend `BlockPredictionRepository.update_verification` to accept `is_correct` and persist both the binary flag and the actual closing price; add an optional `limit` argument to `get_pending_verifications` and return the domain objects instead of booleans.
- Update `BlockVerificationService`: compute `'CORRECT'/'WRONG'` separately from the actual direction, pass all required args to the repository, return the updated model, and expose stats fields (`total_verified`, `correct_count`, …) that match the test expectations. Fix `get_verification_accuracy` to call the repository with the supported `hours` argument (or extend the repository to accept a `date`) and use the corrected stored results.
- Verification: run `./venv/bin/pytest tests/unit/services/test_block_verification_service.py` plus the integration flow to ensure the verification loop is green.

### Phase 3 – Regression hardening & duplication cleanup
- Sweep the test suite for legacy repository/fetcher names and replace them with the canonical APIs introduced in Phases 1–2, eliminating the duplicated mocks that currently model dead methods.
- Document the finalized service contracts in README/QUICK_START so future contributors know which methods to mock, and prune redundant Markdown artifacts that describe superseded phases if they no longer add value.
- Finish with a full `./venv/bin/pytest` run (and any targeted scripts such as `test_phase4_endpoints.py`) to validate the entire repo once the prediction + verification surfaces are aligned.

